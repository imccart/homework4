---
title: "Homework 4 Answer Key"
author: "Econ 470/HLTH 470: Research in Health Economics"
date: ""
header-includes:
  - \usepackage{booktabs}
  - \usepackage{subfig}
output: 
  bookdown::pdf_document2:
    fig_caption: yes
    toc: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, ggplot2, dplyr, lubridate, stringr, readxl, data.table, gdata, scales,
               stargazer, ivpack, kableExtra, modelsummary)
```

```{r, include=FALSE}
load("Hwk4_workspace.Rdata")
```

My answers to the homework questions are described below. As with the previous homework assignments, note that my analysis is in a seperate `R` script. Some of the relevant code is copied in the answer key as well, but the code isn't physically executed in the markdown document. Enjoy!

# Summarize the data

\noindent 1. Remove all SNPs, 800-series plans, and prescription drug only plans (i.e., plans that do not offer Part C benefits). Provide a box and whisker plot showing the distribution of plan counts by county over time. Do you think that the number of plans is sufficient, too few, or too many?


The goal of this graph is to illustrate how many plans are available to an average enrollee in an average county. We remove SNPs, 800-series plans, and Part D only plans in order to focus on a more similar product that is available to everyone. For consistency with the rest of the analysis, I've also removed plans with missing plan IDs, plans in US territories, and plans not operating in an approved service area. The resulting box and whisker plot is provided in Figure \@ref(fig:final-plan-plot), which provides a sense of the distribution of plan counts across counties in each year. From Figure \@ref(fig:final-plan-plot), it is easy to see the expansion in number of plans per county right after 2006 (the passage of the Medicare Modernization Act). 

```{r final-plan-plot, echo=FALSE, out.width="75%", fig.align="center", fig.cap="Average Number of Plans per County"}
final.plan.plot
```



\newpage
\noindent 2. Provide bar graphs showing the distribution of star ratings in 2009, 2012, and 2015. How has this distribution changed over time?


Counts of plans by star rating and year are provided in Figure \@ref(fig:ratings-plot). As is evident from the figure, the distribution of ratings has shifted rightward over time, with many more plans receiving higher ratings.

```{r ratings-plot, echo=FALSE, out.width="75%", fig.align="center", fig.cap="Distribution of Star Ratings over Time"}
ratings.years
```



\newpage
\noindent 3. Plot the average benchmark payment over time. How much has the average benchmark payment risen over the years?

Average MA benchmark rates are presented in Figure \@ref(fig:bench-plot). As we can see, the payment rates have been relatively steady over this time period, with a spike in 2014 and a drop in 2015. Note that 2012 through 2014 is when there were additional quality improvement incentives built into the benchmark payments. Those incentives changed in 2015, consistent with the growth in benchmark payments through 2014 and subsequent decrease.


```{r bench-plot, echo=FALSE, out.width="75%", fig.align="center", fig.cap="Average Medicare Advantage Benchmark Payments, 2008-2015"}
avg.benchmark
```



\newpage
\noindent 4. Plot the average share of Medicare Advantage (relative to all Medicare eligibles) over time. Has Medicare Advantage increased or decreased in popularity? How does this share correlate with benchmark payments?

Average MA market shares are presented in Figure \@ref(fig:ma-share), revealing the steep increase in Medicare Advantage popularity over time. This doesn't seem to be strongly related to Medicare Advantage benchmark payments, however, as the correlation between average benchmark payments and market shares is just `r round(sqrt(summary(share.reg)$r.squared),4)`. So the relationship between these two variables is relatively weak.


```{r ma-share, echo=FALSE, out.width="75%", fig.align="center", fig.cap="Medicare Advantage Market Share (of all Medicare eligibles), 2008-2015"}
ma.share
```



\newpage
# Estimate ATEs
Now let's work on estimating the effects of quality ratings on enrollments using a regression discontinuity design. We'll focus only on 2009, as this is the year in which the star rating running variable is easiest to replicate.<br>

  
\vspace{.2in}
\noindent 1. Calculate the running variable underlying the star rating. Provide a table showing the number of plans that are rounded up into a 3-star, 3.5-star, 4-star, 4.5-star, and 5-star rating.

Note that there are no 5-star plans in 2009, so that's irrelevant in this case. Table \@ref(tab:rounded-count) presents the counts of plans rounded up into each relevant star rating category.


```{r rounded-count, echo=FALSE}
knitr::kable(ma.rounded, 
             col.names=c("Star Rating", "Rounded"),
             caption = "Count of Plans Rounded Up", 
             booktabs = TRUE,
             escape=F,
             align="l",
             format='latex') %>% 
  kable_styling(full_width=F,
                latex_options="hold_position")
```


\newpage
\noindent 2. Using the RD estimator with a bandwidth of 0.125, provide an estimate of the effect of receiving a 3-star versus a 2.5 star rating on enrollments. Repeat the exercise to estimate the effects at 3.5 stars, 4 stars, and 4.5 stars. Summarize your results in a table.

We know from Table \@ref(tab:rounded-count) that there are no plans rounded into the 4.5 star category, so we don't need to worry about that one. For the other categories, we just need to run a local linear regression within the relevant window around each star rating. Results are summarized in Table \@ref(tab:rd-estimates).

```{r rd-estimates, echo=FALSE, results="asis"}
models <- list("3 Stars" = star30, 
               "3.5 Stars" = star35, 
               "4.0 Stars" = star40)
modelsummary(models,
             coef_rename=c("treatTRUE" = "Rounded","score"="Running Score"),
             title="RD Estimates by Star Rating",
             gof_omit='DF|F|Lik|AIC|BIC|Adj') %>%
    kable_styling(latex_options="hold_position")
```



\newpage
\noindent 3. Repeat your results for bandwidhts of 0.1, 0.12, 0.13, 0.14, and 0.15. Show all of the results in a graph. How sensitive are your findings to the choice of bandwidth?

The hardest part about this question is presenting the answers in a graph. There are **lots** of ways to do this in R (as is the case for most things in R). But the easiest way I've found is to use the `broom` package, which allows you to put other `R` objects (like regression results) into a tidy data set, which you can then plot using standard methods. Figure \@ref(fig:rd-h) presents the different RD estimates (from a local linear regression with constant slope terms) across star rating categories and different bandwidth values.


```{r rd-h, echo=FALSE, out.width="75%", fig.align="center", fig.cap="Summary of RD Estimates"}
rd.estimates
```


\newpage
\noindent 4. Examine (graphically) whether contracts appear to manipulate the running variable. In other words, look at the distribution of the running variable before and after the relevent threshold values. What do you find?


Density plots of plans around the 2.75 threshold and the 3.25 thresholds are presented in Figure \@ref(fig:kd-plot). From the distribution around the 2.75 threshold, a couple of points stand out. First, there are very few plans in the left-side of the threshold, meaning very few plans are rounded down to 2.5 stars. We also see that the bulk of plans in this area have raw scores of around 2.85. Looking at the 3.25 threshold, we see a spike in plans just below the threshold.

```{r kd-plot, echo=FALSE, out.width="45%", fig.align="center", fig.cap="Kernel Density Plots around Running Value Thresholds", fig.subcap=c('Around 2.75 threshold', 'Around 3.25 threshold'), fig.asp=1, fig.ncol=2}
kd.running30
kd.running35
```


Ultimately, while there are some apparent differences in the distribution of the raw scores above and below the threshold values, there doesn't appear to be much evidence of "manipulation" of the running variable. In other words, if insurers were manipulating this variable, then they would want to push their scores just above the threshold values in order to move into the higher rating category. We see no evidence of this in the data.


Note that there are much more formal ways of testing for these types of differences, but those are beyond what is expected for this class. If you are interested, please look into the `rdplotdensity` and `rddensity` commands, as well as the `DCdensity` command. I didn't explicitly ask for a formal test in the question, so I didn't do it as part of the answer key either; although there are examples of such formal tests in the slides.



\newpage
\noindent 5. Examine whether plans just above the threshold values have different characteristics than contracts just below the threshold values. Use HMO and Part D status as your plan characteristics.


For this, the standard approach is to use a "love plot" as we did earlier in the semester. In this case, I'm working with just two plan characteristics: 1) whether the plan is an "HMO" type, which is in the "plan_type" variable; and 2) whether the plan offers Part D (prescription drug) coverage. Balance plots are presented in Figure \@ref(fig:love-plots), where we see very large differences in plans rounded down to 2.5 versus those rounded up to 3.0 stars. This perhaps isn't surprising given the lack of data on plans rounded down to 2.5. Conversely, the covariates are more similar among those rounded up to 3.5 versus rounded down to 3 stars.

```{r love-plots, echo=FALSE, out.width="45%", fig.align="center", fig.cap="Balance Plot for around Running Thresholds", fig.subcap=c('Around 2.75 threshold', 'Around 3.25 threshold'), fig.asp=1, fig.ncol=2}
plot.30
plot.35
```


\newpage
\noindent 6. Summarize your findings from 1-5. What is the effect of increasing a star rating on enrollments? Briefly explain your results.

We find relatively small and insignificant effects except in the case of 2.5 versus 3.0 stars. This may suggest that people try to avoid very bad plans (below 3 stars is what CMS designates as low quality), but conditional on having at least an average star rating, individuals may not rely heavilyt on the rating information in their plan choice.



